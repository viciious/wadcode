#!/bin/env python3

#	Copyright (C) 2023 Victor Luchits
#
#	This file is free software; you can redistribute it and/or modify
#	it under the terms of the GNU General Public License as published by
#	the Free Software Foundation; this program is ONLY licensed under
#	version 3 of the License, later versions are explicitly excluded.
#
#	This file is distributed in the hope that it will be useful,
#	but WITHOUT ANY WARRANTY; without even the implied warranty of
#	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#	GNU General Public License for more details.
#
#	You should have received a copy of the GNU General Public License
#	along with this program.  If not, see <https://www.gnu.org/licenses/>.

import sys
import os
import mmap
import json
import hashlib

from MultiCommand import MultiCommand
from BaseCommand import BaseCommand

dirname = ""
content = ""

def diskfn(fn):
    return os.path.join(dirname, fn)

def readu16(f):
    return int.from_bytes(f.read(2), "big", signed=False)

def readu32(f):
    return int.from_bytes(f.read(4), "big", signed=False)

def readu8(f):
    return int.from_bytes(f.read(1), "big", signed=False)

def eu8(v):
    return (v.to_bytes(1, "big", signed=False))

def ebu16(v):
    return (v.to_bytes(2, "big", signed=False))

def ebu32(v):
    return (v.to_bytes(4, "big", signed=False))

def writeu8(f, v):
    f.write(eu8(v))

def writebu16(f, v):
    f.write(ebu16(v))

def writebu32(f, v):
    f.write(ebu32(v))

def readpatch(fin, debug=False):
    with open(fin, "rb") as f:
        width = readu16(f)
        height = readu16(f)
        offs = readu32(f)
        columnofs = []
        columns = []

        for i in range(width):
            columnofs.append(readu16(f))
        
        for i in range(width):
            if debug:
                print(columnofs[i])
            f.seek(columnofs[i])
            while True:
                topdelta = readu8(f)
                length = readu8(f)
                if debug:
                    print("tl", topdelta, length)
                if topdelta == 255 or length == 0:
                    columns.append((255, 255, 0))
                    break
                columns.append((topdelta, length, readu16(f)))
        return (width, height, offs, columnofs, columns)
    return (0, 0, 0, None, None)

def writepatch(fin, width, height, offs, columnofs, columns, mode="wb"):
    with open(fin, mode) as f:
        writebu16(f, width)
        writebu16(f, height)
        writebu32(f, offs)

        for i in range(width):
            writebu16(f, columnofs[i])

        j = 0
        sentinel = False

        for i in range(width):
            while j < len(columns):
                col, j = columns[j], j + 1

                if col[0] == 255:
                    writeu8(f, 255)
                    sentinel = True
                    break
                writeu8(f, col[0])
                writeu8(f, col[1])
                writebu16(f, col[2])

            if not sentinel:
                writeu8(f, 255)
                sentinel = True

def writepixels(fin, pixels):
    with open(fin, "wb") as f:
        f.write(pixels)

def allsubcols(uniqcols):
    res = {}
    for c in uniqcols:
        res[c] = uniqcols[c]
    
    for pixels in uniqcols:
        col = uniqcols[pixels]
        pixels = list(pixels)
        collen = len(pixels)
        if collen == 1:
            continue
        for i in range(collen-1):
            subpixels = tuple(pixels[i+1:])
            if subpixels not in res:
                res[subpixels] = (col[0], collen-i-1, col[2]+i+1)

    return res

def optpatch(fin, fin2):
    origsize = os.path.getsize(fin)
    width, height, offs, columnofs, columns = readpatch(fin)

    uniqcols = {}
    with open(fin2, "rb") as f2:
        pixels = mmap.mmap(f2.fileno(), 0, access=mmap.ACCESS_COPY)
        newofs = 8 + width * 2
        
        j = 0
        for i in range(width):
            coloff, columnofs[i] = columnofs[i], newofs

            while j < len(columns):
                topdelta, length, dataofs = columns[j]

                if topdelta == 255:
                    newofs, j = newofs + 1, j + 1
                    break

                data = tuple(pixels[dataofs:dataofs+length])

                x = uniqcols.get(data)
                if x:
                    if x[2] != dataofs:
                        #if length >= 1:
                        #    saved = saved + length
                        #print("duplicate %s %s %d -> %d" % (topdelta, length, dataofs, x[2]))
                        columns[j] = (columns[j][0], columns[j][1], x[2])
                else:
                    uniqcols[data] = (topdelta, length, dataofs)
                    if length > 1:
                        for i in range(length-1):
                            subpixels = tuple(pixels[dataofs+i+1:dataofs+length])
                            if subpixels not in uniqcols:
                                uniqcols[subpixels] = (topdelta, length-i-1, dataofs+i+1)

                newofs, j = newofs + 4, j + 1

        pixels.close()

    writepatch(fin, width, height, offs, columnofs, columns)

    saved = origsize - os.path.getsize(fin)
    if saved > 0:
        print("%s\t%s" % (fin, saved))
    return saved

def optpixels(fin, fin2):
    size = os.path.getsize(fin2)
    width, height, offs, columnofs, columns = readpatch(fin)
    offsets = {}

    with open(fin2, "rb") as f2:
        pixels = mmap.mmap(f2.fileno(), 0, access=mmap.ACCESS_COPY)

        j = 0
        while j < len(columns):
            topdelta, length, dataofs = columns[j]
            if topdelta != 255:
                if dataofs in offsets:
                    l = offsets[dataofs]
                    if length > l:
                        offsets[dataofs] = length
                else:
                    offsets[dataofs] = length
            j = j + 1
        offsets = [(k,v) for k,v in offsets.items()]
        used = sorted(offsets, key=lambda x: x[0])

        #print(used)
        i = 0
        while i < len(used)-1:
            pos, length = used[i]
            nextpos, nextlength = used[i+1]
            if pos+length > nextpos:
                used = used[0:i] + used[i+1:]
                used[i] = (pos, nextpos+nextlength-pos)
                i = i - 1
            i = i + 1

        newpixels = bytearray()

        offset = 0
        k = 0
        origused = used.copy()
        while k < len(used):
            o, l = used[k]
            oo, ol = origused[k]
            if l > 0 and offset < o:
                j = 0
                while j < len(columns):
                    topdelta, length, dataofs = columns[j]
                    if topdelta != 255 and dataofs >= o:
                        #print("%s -> %s" % (dataofs, dataofs - (o - offset)))
                        columns[j] = (topdelta, length, dataofs - (o - offset))
                    j = j + 1
                j = k
                while j < len(used):
                    used[j] = (used[j][0] - (o - offset), used[j][1])
                    j = j + 1

            newpixels = newpixels + pixels[oo:oo+ol]
            offset = offset + l
            k = k + 1

        saved = 0

        pixels.close()

    writepatch(fin, width, height, offs, columnofs, columns)
    writepixels(fin2, newpixels)

    saved = size - os.path.getsize(fin2)
    if saved > 0:
        print("%s\t%s" % (fin2, saved))
    return saved

def findcontent(content, name):
    ind = 0
    for e in content:
        if "name" in e:
            if e["name"] == name:
                return (e, ind)
        ind = ind + 1
    return (None, -1)

def patchuniqs(fin, fin2):
    origsize = os.path.getsize(fin)
    width, _, _, _, columns = readpatch(fin)

    uniqcols = {}
    with open(fin2, "rb") as f2:
        pixels = mmap.mmap(f2.fileno(), 0, access=mmap.ACCESS_COPY)

        j = 0
        for i in range(width):
            while j < len(columns):
                topdelta, length, dataofs = columns[j]
                j = j + 1

                if topdelta == 255:
                    break

                data = tuple(pixels[dataofs:dataofs+length])
                x = uniqcols.get(data)
                if not x:
                    uniqcols[data] = (topdelta, length, dataofs)

        pixels.close()

    return uniqcols

def mergepatches(pixfn, uniqcols, fin, fin2):
    fullpix, partialpix = 0, 0
    origsize = os.path.getsize(pixfn)
    #print("%s %s" % (pixfn, origsize))
    width, height, offs, columnofs, columns = readpatch(fin)

    subcols = allsubcols(uniqcols)
    #subcols = uniqcols
    with open(pixfn, "ab") as f3:
        with open(fin2, "rb") as f2:
            pixels = mmap.mmap(f2.fileno(), 0, access=mmap.ACCESS_COPY)

            j = 0
            for i in range(width):
                while j < len(columns):
                    topdelta, length, dataofs = columns[j]

                    if topdelta == 255:
                        columns[j] = (topdelta, length, origsize)
                        j = j + 1
                        break

                    data = tuple(pixels[dataofs:dataofs+length])
                    x = subcols.get(data)
                    if not x:
                        uniqcols[data] = (topdelta, length, origsize)
                        subcols[data] = (topdelta, length, origsize)
                        data = bytearray(data)
                        f3.write(data)
                        columns[j] = (topdelta, length, origsize)
                        origsize += len(data)
                    else:
                        if uniqcols.get(data):
                            fullpix += length
                        else:
                            partialpix += length
                        columns[j] = (topdelta, length, x[2])

                    j = j + 1

            pixels.close()

    writepatch(fin, width, height, offs, columnofs, columns)

    return (uniqcols, fullpix, partialpix)

def comparepatches(cols, cols2):
    pix = 0
    for col in cols2:
        if col in cols:
            pix += len(col)
    return pix

def crunchpatches(uniqs = {}, minthreshold=0, maxthreshold=0, skipsize=False):
    totalpix = 0
    totalcnt = 0

    _, s_start = findcontent(content, "S_START")
    _, s_end = findcontent(content, "S_END")

    sha1uniqs = {}
    sha1s = [''] * len(content)

    i = s_start + 1
    while i < s_end:
        fn = content[i]["filename"]

        patch = diskfn(fn)
        pixels = diskfn(content[i+1]["filename"])

        j = 0
        for tfn in [patch, pixels]:
            sha1 = hashlib.sha1()
            data = b""
            with open(tfn, "rb") as f:
                data = f.read()
            sha1 = hashlib.sha1()
            sha1.update(data)
            sha1 = sha1.hexdigest()

            if sha1 not in sha1uniqs:
                sha1uniqs[sha1] = 1
            else:
                sha1uniqs[sha1] = sha1uniqs[sha1] + 1
            sha1s[i+j] = sha1
            j = j + 1

        if fn not in uniqs:
            cols = patchuniqs(patch, pixels)
            uniqs[fn] = (False, cols, [])

        i = i + 2

    i = s_start + 1

    while i < s_end:
        fn = content[i]["filename"]
        pixfn = diskfn(content[i+1]["filename"])
        pixfnsize = os.path.getsize(pixfn)

        mapped, cols, pixfns = uniqs[fn]
        if mapped:
            i = i + 2
            continue

        bestpix = minthreshold-1
        bestmatch = -1
        bestcols = None

        j = i + 2
        while j < s_end:
            fn2 = content[j]["filename"]
            pixfn2 = diskfn(content[j+1]["filename"])
            pixfnsize2 = os.path.getsize(pixfn2)

            # guard against ridiculously big lumps
            if pixfnsize+pixfnsize2 > 62*1024:
                j = j + 2
                continue

            if fn2 == fn:
                j = j + 2
                continue
            mapped, cols2, pixfns2 = uniqs[fn2]
            if mapped or len(pixfns2) > 0:
                j = j + 2
                continue

            pix = comparepatches(cols, cols2)
            if pix > bestpix:
                if skipsize or sha1uniqs[sha1s[j]] == 1 or pix > os.path.getsize(diskfn(fn2)):
                    bestpix = pix
                    bestmatch = j
                    bestcols = cols2

            j = j + 2

        if bestpix >= minthreshold and (maxthreshold == 0 or bestpix < maxthreshold):
            totalcnt = totalcnt + 1
            fn2 = content[bestmatch]["filename"]
            pixfn2 = diskfn(content[bestmatch+1]["filename"])

            cols, fullpix, partialpix = mergepatches(pixfn, cols, diskfn(fn2), pixfn2)
            print("merged %s into %s: %s/%s pixels in full/partial columns" % (fn2, fn, fullpix, partialpix))

            totalpix = totalpix + fullpix + partialpix

            pixfns.append(pixfn2)
            pixfns = pixfns + pixfns2

            uniqs[fn2] = (True, bestcols, [])
            uniqs[fn] = (False, cols, pixfns)

            # copy pixels
            with open(pixfn, "rb") as fin:
                #print("%s -> %s" % (pixfn, pixfns))
                pixels = fin.read()
                for pixfn2 in pixfns:
                    with open(pixfn2, "wb") as fout:
                        fout.write(pixels)

        i = i + 2

    print("total\t\t%s" % totalpix)
    return uniqs, totalcnt

def optimizepatches():
    totalpix = 0
    _, s_start = findcontent(content, "S_START")
    _, s_end = findcontent(content, "S_END")

    i = s_start + 1
    while i < s_end:
        fn = content[i]["filename"]
        patch = diskfn(fn)
        pixels = diskfn(content[i+1]["filename"])

        print("optimizing %s" % fn)

        totalpix += optpatch(patch, pixels)
        totalpix += optpixels(patch, pixels)

        i = i + 2
    print("total\t%s" % totalpix)

def squishpatches(uniqs = {}):
    totalsaved = 0

    _, s_start = findcontent(content, "S_START")
    _, s_end = findcontent(content, "S_END")

    sha1uniqs = {}
    sha1s = [''] * len(content)

    i = s_start + 1
    while i < s_end:
        fn = content[i]["filename"]
        pixels = diskfn(content[i+1]["filename"])

        for tfn in [pixels]:
            sha1 = hashlib.sha1()
            data = b""
            with open(tfn, "rb") as f:
                data = f.read()
            sha1 = hashlib.sha1()
            sha1.update(data)
            sha1 = sha1.hexdigest()

            if sha1 not in sha1uniqs:
                sha1uniqs[sha1] = [i]
            else:
                sha1uniqs[sha1].append(i)
            sha1s[i] = sha1

        i = i + 2

    for k in sha1uniqs:
        patches = sha1uniqs[k]
        if len(patches) < 2:
            continue

        batch = list()
        coluniqs = {}

        n = len(patches) - 1
        while n >= 0:
            patch = patches[n]
            fn = content[patch]["filename"]
            width, height, offs, columnofs, columns = readpatch(diskfn(fn))
            ofs = 8 + width * 2
            patchlen = ofs

            j = 0
            rawcolumns = []
            for i in range(width):
                raw = bytearray()
                sentinel = False

                while j < len(columns):
                    col, j = columns[j], j + 1

                    if col[0] == 255:
                        raw.append(255)
                        sentinel = True
                        break
                    raw.append(col[0])
                    raw.append(col[1])
                    raw.extend(ebu16(col[2]))

                if not sentinel:
                    raw.append(255)
                    sentinel = True

                data = tuple(raw)
                if data not in coluniqs:
                    coluniqs[data] = (n, patchlen)

                #    # add partial columns
                #    for l in range(len(raw)-2):
                #        data = tuple(raw[l+1:])
                #        if data not in coluniqs:
                #            coluniqs[data] = (n, patchlen+l+1)

                    patchlen = patchlen + len(raw)
                ofs = ofs + len(raw)

                rawcolumns.append(raw)

            batch.append((width, height, offs, columnofs, rawcolumns, patch, ofs, patchlen, 0))

            n = n - 1
            totalsaved = totalsaved + (ofs - patchlen)

        # reverse order
        patches = []
        bloblen = 0
        for n in range(len(batch)):
            patch = list(batch.pop())
            patch[8] = bloblen
            patchlen = patch[7]
            if patchlen & 1:
                patchlen = patchlen + 1
            bloblen = bloblen + patchlen
            patches.append(tuple(patch))

        # distances from each patch to other patches
        dists = [0]*len(patches)
        for n in range(len(patches)):
            pos = patches[n][8]
            dists[n] = [0]*len(patches)
            for j in range(len(patches)):
                dists[n][j] = patches[j][8] - pos

        for n, patch in enumerate(patches):
            fn = content[patch[5]]["filename"]
            fin = diskfn(fn)

            with open(fin, "wb") as f:
                m = n
                while m < len(patches):
                    written = {}
                    patch = patches[m]
                    width, height, offs, columnofs, rawcolumns, _, _, patchlen, _ = patch

                    flen = f.tell()
                    writebu16(f, width)
                    writebu16(f, height)
                    writebu32(f, offs)

                    for i in range(width):
                        data = tuple(rawcolumns[i])
                        u = coluniqs[data]
                        colofs = dists[m][u[0]]
                        if colofs & 1:
                            colofs = colofs + 1
                        colofs = colofs + u[1]
                        writebu16(f, colofs)

                    for i in range(width):
                        data = tuple(rawcolumns[i])
                        u = coluniqs[data]
                        if u[0] == m and data not in written:
                            f.write(rawcolumns[i])
                            written[data] = True

                    flen = f.tell() - flen
                    if flen != patchlen:
                        print(flen, patchlen)
                        sys.exit(1)
                    if patchlen & 1:
                        writeu8(f, 255)
                    m = m + 1

    print("total\t\t%s" % totalsaved)

class CommandPatchOpt(BaseCommand):
    def __init__(self, cmd, args):
        global dirname
        global content

        BaseCommand.__init__(self, cmd, args)

        dirname = args.dir
        with open(os.path.join(dirname, "content.json")) as f:
            content = json.load(f)
        dirname = os.path.join(dirname, "files")

class CommandOptimize(CommandPatchOpt):
    def __init__(self, cmd, args):
        CommandPatchOpt.__init__(self, cmd, args)

        print("Optimizing patches...")

        optimizepatches()

class CommandCrunch(CommandPatchOpt):
    def __init__(self, cmd, args):
        CommandPatchOpt.__init__(self, cmd, args)

        print("Crunching patches...")

        uniqs = {}
        minthreshold = 8000
        while True:
            uniqs, cnt = crunchpatches(uniqs, minthreshold=minthreshold)
            if minthreshold == 16:
                break
            if cnt != 0:
                continue
            minthreshold = int(minthreshold * 0.75)
            if minthreshold < 16:
                minthreshold = 16

class CommandSquish(CommandPatchOpt):
    def __init__(self, cmd, args):
        CommandPatchOpt.__init__(self, cmd, args)

        print("Squishing patches...")

        squishpatches()

mc = MultiCommand()

def genparser(parser):
	parser.add_argument("dir", metavar = "directory", type = str, help = "Input directory that should be compiled to a WAD.")
mc.register("optimize", "Remove redundant data from individual sprite patches", genparser, action = CommandOptimize)

def genparser(parser):
	parser.add_argument("dir", metavar = "directory", type = str, help = "Input directory that should be compiled to a WAD.")
mc.register("crunch", "Crunch patches, removing redundant data across multiple files", genparser, action = CommandCrunch)

def genparser(parser):
	parser.add_argument("dir", metavar = "directory", type = str, help = "Input directory that should be compiled to a WAD.")
mc.register("squish", "Crunch patches, removing redundant data across multiple files", genparser, action = CommandSquish)

mc.run(sys.argv[1:])
